
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Blog dealing about Data</title>
  <meta name="author" content="Romain NIO">

  
  <meta name="description" content="The Hadoop native librairies are compiled for 32 bits plateforms. If you are using Hadoop on x64, you have probably been faced to the following issue &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://www.rnio.me/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
   <link href="/stylesheets/data-table.css" media="screen, projection" rel="stylesheet" type="text/css" />
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="/javascripts/ender.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="Blog dealing about Data" type="application/atom+xml">
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">

<!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href='http://fonts.googleapis.com/css?family=Lato:400,100,100italic,300,300italic,400italic,700,700italic,900,900italic' rel='stylesheet' type='text/css'>
<link href='http://fonts.googleapis.com/css?family=Lora:400,400italic,700,700italic' rel='stylesheet' type='text/css'>
<link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:200,300,400,500,600,700,900' rel='stylesheet' type='text/css'>


	<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$$','$$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>




  

  <style>html{background: url(/images/background.png) no-repeat center center fixed;-webkit-background-size: cover;-moz-background-size: cover;-o-background-size: cover;background-size: cover;}</style>
</head>

<body   >
  <header role="banner"><hgroup>
</hgroup>

</header>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/06/16/build-hadoop-native-librairies/">Build Hadoop Native Librairies</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-06-16T19:21:41+02:00" pubdate data-updated="true"></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>The Hadoop native librairies are compiled for 32 bits plateforms. If you are using Hadoop on x64, you have probably been faced to the following issue :</p>

<pre><code> WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 
</code></pre>

<p>For performances purposes, it better to recompile those libraries according to your plateform.</p>

<p>It’s a good idea to compile on the same architecture than your Hadoop production plateform. Of course, avoid any compilations on your production server Not sure that hadoop native librairies are compiled for 32 bits plateform ? You can check that with the following command :</p>

<pre><code>file $HADOOP_HOME/lib/native/libhadoop.so.1.0.0
</code></pre>

<p>Here the result :</p>

<pre><code>file libhadoop.so.1.0.0: ELF 32-bit
</code></pre>

<h2 id="download-source">Download Source</h2>

<p>Visit http://mirrors.ircam.fr/pub/apache/hadoop/common/ and find the tarball of your Hadoop version. Download it :</p>

<pre><code>wget http://mirrors.ircam.fr/pub/apache/hadoop/common/hadoop-2.4.1/hadoop-2.4.1.tar.gz
</code></pre>

<p>Install dependencies :</p>

<pre><code>sudo apt-get install cmake autoconf automake libtool gcc zlib1g-dev pkg-config libssl-dev openssl gcc g++ make maven zlib zlib1g-dev libcurl4-o
</code></pre>

<p>Install protobuf :</p>

<pre><code>wget https://protobuf.googlecode.com/files/protobuf-2.5.0.tar.gz gunzip protobuf-2.5.0.tar.gz
tar -xvf protobuf-2.5.0.tar
cd protobuf-2.5.0
sudo ./configure --prefix=/usr sudo make
sudo make install
</code></pre>

<h2 id="compile-hadoop">Compile Hadoop</h2>
<p>Unzip you tarball :</p>

<pre><code>tar -xzf hadoop-2.4.1-src.tar.gz
</code></pre>

<p>Enter in your folder :</p>

<pre><code>cd hadoop-2.4.1-src/
</code></pre>

<p>Set your environment :</p>

<pre><code>export Platform=x64
Compile :
mvn package -Pdist,native -DskipTests -Dtar
</code></pre>

<p>If you face issues while compiling, google is your friend ;). If all is OK, you will have this kind of output :</p>

<pre><code>[INFO] ------------------------------------------------------------- [INFO] BUILD SUCCESS
[I------------------------------------------------------------------ [INFO] Total time: 5:27.684s
[INFO] Finished at: Wed Jul 02 19:33:51 CEST 2014
[INFO] Final Memory: 165M/834M
[INFO] -----------------------------------------------------------------------
</code></pre>

<p>You can find the librairies in this folder :</p>

<pre><code>cd ./hadoop-dist/target/hadoop-2.4.1/lib/native
</code></pre>

<p>We can see all built librairies (“ls ­lh”) :</p>

<pre><code>-rw-r--r-- 1 hadoop hadoop 1.1M Jul 2 19:07 libhadoop.a
lrwxrwxrwx 1 hadoop hadoop 18 Jul 2 19:07 libhadoop.so -&gt; libhadoop.so.1.0.0 -rwxr-xr-x 1 hadoop hadoop 650K Jul 2 19:07 libhadoop.so.1.0.0
-rw-r--r-- 1 hadoop hadoop 1.4M Jul 2 19:07 libhadooppipes.a
-rw-r--r-- 1 hadoop hadoop 421K Jul 2 19:07 libhadooputils.a
-rw-r--r-- 1 hadoop hadoop 373K Jul 2 19:07 libhdfs.a
lrwxrwxrwx 1 hadoop hadoop 16 Jul 2 19:07 libhdfs.so -&gt; libhdfs.so.0.0.0 -rwxr-xr-x 1 hadoop hadoop 245K Jul 2 19:07 libhdfs.so.0.0.0
</code></pre>

<p>At this step, you can check the plateform of the librairies :</p>

<pre><code>file libhadoop.so.1.0.0
</code></pre>

<p>The result seems to be OK :</p>

<pre><code>libhadoop.so.1.0.0: ELF 64-bit LSB shared object, x86-64
</code></pre>

<p>save them and archive this package :</p>

<pre><code>tar -cvzf hadoop-native-libraries-2.4.1.tgz *
</code></pre>

<h2 id="copy-librairies-on-your-cluster">Copy librairies on your cluster</h2>

<p>This step need to be reproduced for namenode and datanode.
You just need to copy all those file in $HADOOP_HOME/lib/native (eg : /usr/local/hadoop/lib/native) :</p>

<pre><code>$ rsync hadoop-native-libraries-2.4.1.tgz &lt;your_hadoop_production_server&gt;:/usr/local/hadoop/lib/native/
</code></pre>

<p>Enter in your Hadoop home (eg : /usr/local/hadoop/lib/native)</p>

<pre><code>$ cd $HADOOP_HOME/lib/native
</code></pre>

<p>Extract archives :</p>

<pre><code>$ tar -xzf hadoop-native-libraries-2.4.1.tgz
</code></pre>

<h2 id="configure-your-environment">Configure your environment</h2>

<p>You probably have a specific unix user for your hadoop cluster. Add those lines in your ~/.bashrc (or coure, edit paths according to your configuration):</p>

<pre><code>export HADOOP_INSTALL=/usr/local/hadoop
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_INSTALL/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_COMMON_LIB_NATIVE_DIR $HADOOP_OPTS"
</code></pre>

<h2 id="stop-and-restart-hadoop">Stop and restart Hadoop</h2>

<p>Stop cluster :</p>

<pre><code>./stop-dfs.sh 
./stop-yarn.sh
</code></pre>

<p>Source again your bashrc :</p>

<pre><code>source ~/.bashrc
</code></pre>

<p>Start Hadoop :</p>

<pre><code>./start-dfs.sh
./start-yarn.sh
</code></pre>

<p>Test that the message disappears :</p>

<pre><code>$ hadoop fs -ls /user/hadoop
Found 1 item
-rwxr-xr-x 1 hadoop supergroup 8 2014-07-01 14:06 /user/hadoop/toto.txt
</code></pre>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/06/05/split-large-file-unix-in-bash-command-line/">Split Large File in Bash</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-06-05T23:44:18+02:00" pubdate data-updated="true"></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>When you are dealing with large file, it’s complicated to share or manipulate them.</p>

<p>On linux the split command can be useful for you.</p>

<p>Basic Usage :</p>

<pre><code>$ split [-l] [-b] filename prefix
</code></pre>

<p>For example, if you want to split a large file named “clients.csv” and create files with 100k records / file, apply the following command:</p>

<pre><code>$ split -l 100000 clients.csv splitted_clients-
“splitted_client” is the prefix applied to each generated files
</code></pre>

</div>
  
  


    </article>
  
  <div class="pagination">
    
    <a href="/blog/archives/"> Archives </a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <ul id="recent_posts">
      <li class="post">
      <a href="http://www.rnio.me" alt="Home"><img src="/images/Home.png"></a>
      <a href="http://www.rnio.me/archives/" alt="Archives"><img src="/images/Calendar.png"></a>
      <a href="mailto:" alt="E-Mail"><img src="/images/Envelope.png"></a>
      <a href="http://www.rnio.me/atom.xml" alt="subscribe feed"><img src="/images/rss_big.png"></a>
      </li>
  </ul>
</section>
<section id="titles">
  <a href="http://www.rnio.me" title="Blog dealing about Data"><img id="logo" src="http://www.rnio.me/images/" /></a>
  <h1 id="site_title"><a href="http://www.rnio.me" title="Blog dealing about Data">Blog dealing about Data</a></h1>
  <h3 id="site_subtitle">Romain NIO  - Data team Oscaro.com</h3>
</section>

<section id="menu">
  <ul>
    <li><i class="fa fa-home fa-lg"></i><a href="http://www.rnio.me"> Home </a></li>
    <li><i class="fa fa-calendar fa-lg"></i><a href="http://www.rnio.me/blog/archives/"> Archives </a></li>
    <li><i class="fa fa-user fa-lg"></i><a href="http://www.rnio.me/about/"> About </a></li>
    <li><i class="fa fa-rss fa-lg"></i><a href="http://www.rnio.me/atom.xml"> Feed </a></li>
  </ul>
</section>

<section id="social">
  

  

  

  

  

  
</section>


<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2015/06/16/build-hadoop-native-librairies/">Build hadoop native librairies</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/06/05/split-large-file-unix-in-bash-command-line/">Split large file in bash</a>
      </li>
    
  </ul>
</section>





  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2015 - Romain NIO -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
